{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6dc5671-6db6-49f9-a982-52477b06acf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:25:38.879291Z",
     "iopub.status.busy": "2024-05-29T18:25:38.878708Z",
     "iopub.status.idle": "2024-05-29T18:25:38.883653Z",
     "shell.execute_reply": "2024-05-29T18:25:38.882778Z",
     "shell.execute_reply.started": "2024-05-29T18:25:38.879256Z"
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = \"https://dryad-assetstore-merritt-west.s3.us-west-2.amazonaws.com/ark%3A/13030/m53853vd%7C6%7Cproducer/competitionData.tar.gz?response-content-disposition=attachment%3B%20filename%3DcompetitionData.tar.gz&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA2KERHV5E3OITXZXC%2F20240517%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20240517T074950Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=89d4fc9165caa9fe8baae163f05912df4836f7418c8e6902dc4ddf99da1cb1ac\"\n",
    "# local_filename = \"competitionData.tar.gz\"\n",
    "# response = requests.get(url)\n",
    "# if response.status_code == 200:\n",
    "#     with open(local_filename, 'wb') as f:\n",
    "#         f.write(response.content)\n",
    "#     print(f\"File '{local_filename}' has been downloaded successfully.\")\n",
    "# else:\n",
    "#     print(f\"Failed to download file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cc8551-807a-4afe-be10-5e650f0f8d03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:25:38.885644Z",
     "iopub.status.busy": "2024-05-29T18:25:38.885002Z",
     "iopub.status.idle": "2024-05-29T18:25:38.888590Z",
     "shell.execute_reply": "2024-05-29T18:25:38.887765Z",
     "shell.execute_reply.started": "2024-05-29T18:25:38.885605Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# filename = \"competitionData.tar.gz\"\n",
    "# with tarfile.open(filename, 'r:gz') as tar:\n",
    "#     tar.extractall()\n",
    "# print(\"The.tar.gz file has been uncompressed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e40b8a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:25:38.890103Z",
     "iopub.status.busy": "2024-05-29T18:25:38.889680Z",
     "iopub.status.idle": "2024-05-29T18:25:41.851146Z",
     "shell.execute_reply": "2024-05-29T18:25:41.850502Z",
     "shell.execute_reply.started": "2024-05-29T18:25:38.890103Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from transformers.utils import ModelOutput\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config, T5EncoderModel\n",
    "from pytorch_tcn import TCN\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3bf3fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:25:41.853142Z",
     "iopub.status.busy": "2024-05-29T18:25:41.852571Z",
     "iopub.status.idle": "2024-05-29T18:25:41.865449Z",
     "shell.execute_reply": "2024-05-29T18:25:41.864642Z",
     "shell.execute_reply.started": "2024-05-29T18:25:41.853119Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_process_data(directory, partitions):\n",
    "    partitioned_data = {}\n",
    "\n",
    "    # Store temporary data for all partitions\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_lengths = []\n",
    "    all_partition_info = []\n",
    "\n",
    "    for partition in partitions:\n",
    "        partition_dir = os.path.join(directory, partition)\n",
    "        files = sorted([f for f in os.listdir(partition_dir) if f.endswith('.mat')])\n",
    "\n",
    "        input_features = []\n",
    "        transcriptions = []\n",
    "        frame_lens = []\n",
    "        all_block_idxs = []\n",
    "\n",
    "        for file in tqdm(files, desc=f\"Loading and processing {partition}\"):\n",
    "            path = os.path.join(partition_dir, file)\n",
    "            session_data = load_features_and_normalize(path)\n",
    "\n",
    "            input_features.extend(session_data['inputFeatures'])\n",
    "            transcriptions.extend(session_data['transcriptions'])\n",
    "            frame_lens.extend(session_data['frameLens'])\n",
    "            all_block_idxs.extend(session_data['blockIdx'])\n",
    "\n",
    "        # Normalize each partition's features\n",
    "        combined_features, block_means, block_stds = block_normalize(input_features, all_block_idxs)\n",
    "\n",
    "        # Append partition data for global scaling and PCA\n",
    "        all_features.append(combined_features)\n",
    "        all_labels.extend(transcriptions)\n",
    "        all_lengths.extend(frame_lens)\n",
    "        all_partition_info.append((partition, len(transcriptions)))\n",
    "\n",
    "    # Combine features from all partitions\n",
    "    combined_features = np.vstack(all_features)\n",
    "\n",
    "    # Global scaling\n",
    "    scaler = RobustScaler()\n",
    "    combined_features = scaler.fit_transform(combined_features)\n",
    "\n",
    "    # PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=0.95)\n",
    "    combined_features = pca.fit_transform(combined_features)\n",
    "\n",
    "    # Distribute features back to partitions\n",
    "    start_index = 0\n",
    "    for partition, size in all_partition_info:\n",
    "        end_index = start_index + size\n",
    "        partitioned_data[partition] = {\n",
    "            'features': combined_features[start_index:end_index],\n",
    "            'label': all_labels[start_index:end_index],\n",
    "            'sen_len': all_lengths[start_index:end_index]\n",
    "        }\n",
    "        start_index = end_index\n",
    "\n",
    "    return partitioned_data\n",
    "\n",
    "def block_normalize(input_features, block_idxs):\n",
    "    unique_blocks = np.unique(block_idxs)\n",
    "    combined_features = []\n",
    "    \n",
    "    for block in unique_blocks:\n",
    "        block_indices = [i for i, x in enumerate(block_idxs) if x == block]\n",
    "        block_feats = np.vstack([input_features[i] for i in block_indices])\n",
    "        \n",
    "        block_mean = np.mean(block_feats, axis=0)\n",
    "        block_std = np.std(block_feats, axis=0)\n",
    "        \n",
    "        normalized_feats = [(input_features[i] - block_mean) / (block_std + 1e-8) for i in block_indices]\n",
    "        combined_features.extend(normalized_feats)\n",
    "\n",
    "    return np.vstack(combined_features), None, None\n",
    "\n",
    "def load_features_and_normalize(sessionPath):\n",
    "    dat = scipy.io.loadmat(sessionPath)\n",
    "\n",
    "    input_features = []\n",
    "    transcriptions = []\n",
    "    frame_lens = []\n",
    "    block_idxs = []\n",
    "    n_trials = dat['sentenceText'].shape[0]\n",
    "    blockIdx = np.squeeze(dat['blockIdx'])\n",
    "\n",
    "    # Collect area 6v tx1 and spikePow features\n",
    "    for i in range(n_trials):\n",
    "        features = np.concatenate([dat['tx1'][0, i][:, 0:128], dat['spikePow'][0, i][:, 0:128]], axis=1)\n",
    "        sentence_len = features.shape[0]\n",
    "        sentence = dat['sentenceText'][i].strip().lower()\n",
    "\n",
    "        input_features.append(features)\n",
    "        transcriptions.append(sentence)\n",
    "        frame_lens.append(sentence_len)\n",
    "        block_idxs.append(blockIdx[i])\n",
    "\n",
    "    session_data = {\n",
    "        'inputFeatures': input_features,\n",
    "        'transcriptions': transcriptions,\n",
    "        'frameLens': frame_lens,\n",
    "        'blockIdx': block_idxs\n",
    "    }\n",
    "\n",
    "    return session_data\n",
    "\n",
    "def preview_data(results):\n",
    "    for partition in results:\n",
    "        print(f\"--- {partition.upper()} Partition ---\")\n",
    "        features = results[partition]['features']\n",
    "        transcriptions = results[partition]['label']\n",
    "        frame_lengths = results[partition]['sen_len']\n",
    "        \n",
    "        # Creating DataFrames for better visual representation\n",
    "        features_df = pd.DataFrame(features)\n",
    "        transcriptions_df = pd.DataFrame(transcriptions, columns=['Transcription'])\n",
    "        frame_lengths_df = pd.DataFrame(frame_lengths, columns=['Sentence Length'])\n",
    "        \n",
    "        # Printing shape and size details\n",
    "        print(\"Features Details:\")\n",
    "        print(f\"Shape: {features_df.shape}, Size: {features_df.size}\")\n",
    "        print(\"Features Preview:\")\n",
    "        print(features_df.head())\n",
    "        \n",
    "        print(\"Label Details:\")\n",
    "        print(f\"Shape: {transcriptions_df.shape}, Size: {transcriptions_df.size}\")\n",
    "        print(\"Label Preview:\")\n",
    "        print(transcriptions_df.head())\n",
    "        \n",
    "        print(\"Sentence Lengths Details:\")\n",
    "        print(f\"Shape: {frame_lengths_df.shape}, Size: {frame_lengths_df.size}\")\n",
    "        print(\"Sentence Lengths Preview:\")\n",
    "        print(frame_lengths_df.head())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20eeab9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:25:41.866674Z",
     "iopub.status.busy": "2024-05-29T18:25:41.866266Z",
     "iopub.status.idle": "2024-05-29T18:27:13.390973Z",
     "shell.execute_reply": "2024-05-29T18:27:13.390207Z",
     "shell.execute_reply.started": "2024-05-29T18:25:41.866649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and processing train: 100%|██████████| 24/24 [00:04<00:00,  4.84it/s]\n",
      "Loading and processing test: 100%|██████████| 24/24 [00:00<00:00, 96.25it/s]\n",
      "Loading and processing competitionHoldOut: 100%|██████████| 15/15 [00:00<00:00, 46.02it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'competitionData'\n",
    "partitions = ['train', 'test', 'competitionHoldOut']\n",
    "partitioned_data = load_and_process_data(data_dir, partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d2e62d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:27:13.391900Z",
     "iopub.status.busy": "2024-05-29T18:27:13.391698Z",
     "iopub.status.idle": "2024-05-29T18:27:13.416227Z",
     "shell.execute_reply": "2024-05-29T18:27:13.415425Z",
     "shell.execute_reply.started": "2024-05-29T18:27:13.391889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAIN Partition ---\n",
      "Features Details:\n",
      "Shape: (8800, 123), Size: 1082400\n",
      "Features Preview:\n",
      "         0          1         2         3         4         5         6    \\\n",
      "0  -6.978477   3.589440  9.083821  4.696966  5.081369 -1.942908  2.445167   \n",
      "1  -6.942856   4.718402  9.565770  5.147847  5.552409 -1.592359  4.563524   \n",
      "2  -6.162521  15.191476 -6.775521  6.034155 -6.914832 -3.912339  3.322587   \n",
      "3  -7.094923  -0.951291  7.753854  4.699582  2.685992  5.829111  1.576679   \n",
      "4  30.155407  -4.286747  7.774179  4.928823  3.108494 -2.292313  3.188529   \n",
      "\n",
      "        7         8         9    ...       113       114       115       116  \\\n",
      "0  0.618766 -3.260455 -0.226782  ...  1.098188 -0.281921 -0.480892  0.662092   \n",
      "1  0.616387 -1.932959  1.895795  ... -0.324113  1.787536 -2.580184  0.218130   \n",
      "2 -1.705997 -1.828976  2.358329  ...  0.966154 -1.191206 -1.623980  0.675462   \n",
      "3 -0.569604  1.946202 -1.033215  ...  0.523643 -0.538616 -0.255343  0.756173   \n",
      "4 -0.131116 -4.278669  1.251366  ...  0.441433  0.109235 -1.367879  0.139098   \n",
      "\n",
      "        117       118       119       120       121       122  \n",
      "0 -0.249557 -0.162507  1.363681  0.525482 -0.064308 -0.181537  \n",
      "1  0.073244  0.483074 -1.052730  1.837369  0.633072  0.130316  \n",
      "2 -0.144490  0.195338  1.258911 -0.749111  0.441786  0.561422  \n",
      "3 -1.150757  0.016763  0.154147  0.247164 -0.073701 -0.706406  \n",
      "4 -1.813034  1.314956  0.852921 -1.724023 -0.462964 -0.864212  \n",
      "\n",
      "[5 rows x 123 columns]\n",
      "Label Details:\n",
      "Shape: (8800, 1), Size: 8800\n",
      "Label Preview:\n",
      "                                       Transcription\n",
      "0   nuclear rockets can destroy airfields with ease.\n",
      "1  the best way to learn is to solve extra problems.\n",
      "2  the spray will be used in first division match...\n",
      "3  our experiment's positive outcome was unexpected.\n",
      "4             alimony harms a divorced man's wealth.\n",
      "Sentence Lengths Details:\n",
      "Shape: (8800, 1), Size: 8800\n",
      "Sentence Lengths Preview:\n",
      "   Sentence Length\n",
      "0              478\n",
      "1              446\n",
      "2              554\n",
      "3              461\n",
      "4              348\n",
      "\n",
      "\n",
      "--- TEST Partition ---\n",
      "Features Details:\n",
      "Shape: (880, 123), Size: 108240\n",
      "Features Preview:\n",
      "         0          1          2         3          4         5         6    \\\n",
      "0  29.925991  -8.549191   6.416126  4.209752   0.925595  5.022741 -1.439432   \n",
      "1  -7.178763  -0.924586   7.816775  4.477262   3.532935 -1.928437  3.617826   \n",
      "2  -6.064190  20.575445  13.733198  7.144097 -17.818899 -5.406638  0.565368   \n",
      "3  -6.496464  11.094273  11.089665  6.183554  -6.820768 -3.504071  2.926076   \n",
      "4  -6.980933   2.636842   8.561526  5.067942  -9.616213 -4.293016  1.446021   \n",
      "\n",
      "        7         8         9    ...       113       114       115       116  \\\n",
      "0  0.317201 -2.712145 -0.879036  ...  0.326947  0.413872 -0.373070 -0.330382   \n",
      "1  2.263483 -2.838897  0.290708  ... -0.157273  1.323265 -0.172608 -1.834413   \n",
      "2  0.897746 -2.584358 -1.208995  ...  0.370853 -0.398459 -0.022077  0.089527   \n",
      "3  1.105282 -3.303642 -0.124249  ...  1.438047 -1.015224 -0.038695 -0.591068   \n",
      "4 -0.488151 -2.273823 -0.643935  ...  0.374905  0.754805 -0.519509  0.513066   \n",
      "\n",
      "        117       118       119       120       121       122  \n",
      "0  0.305534  0.609790 -0.117605  0.177504  1.090414 -0.188866  \n",
      "1  1.007833 -0.685501  2.640432  1.251200 -1.320918  0.319175  \n",
      "2  1.601041 -0.256685  0.473074  0.467793 -0.867468 -0.196932  \n",
      "3 -0.411452 -0.868333  2.729557  0.371495 -0.110476  1.602976  \n",
      "4  0.228232 -0.969611  0.625778 -0.021091  0.097325 -0.340695  \n",
      "\n",
      "[5 rows x 123 columns]\n",
      "Label Details:\n",
      "Shape: (880, 1), Size: 880\n",
      "Label Preview:\n",
      "                                       Transcription\n",
      "0                            theocracy reconsidered.\n",
      "1         rich purchased several signed lithographs.\n",
      "2          so rules we made, in unabashed collusion.\n",
      "3  lori's costume needed black gloves to be compl...\n",
      "4  the tooth fairy forgot to come when roger's to...\n",
      "Sentence Lengths Details:\n",
      "Shape: (880, 1), Size: 880\n",
      "Sentence Lengths Preview:\n",
      "   Sentence Length\n",
      "0              254\n",
      "1              386\n",
      "2              457\n",
      "3              576\n",
      "4              672\n",
      "\n",
      "\n",
      "--- COMPETITIONHOLDOUT Partition ---\n",
      "Features Details:\n",
      "Shape: (1200, 123), Size: 147600\n",
      "Features Preview:\n",
      "        0          1          2         3          4         5         6    \\\n",
      "0 -5.879079  24.352137  14.988941  7.403085  -2.498342 -2.800221  3.953157   \n",
      "1 -6.036600  16.403234  -6.758955  6.994838  -7.757525  4.671894  3.544151   \n",
      "2 -5.234031  27.162689 -23.390390  8.092753 -19.254118 -4.960675  6.434055   \n",
      "3 -6.603025   9.276491  10.595097  6.244679  -7.721188 -3.362144  4.642231   \n",
      "4 -6.606721   4.565958 -10.004073  4.691550   2.692525  5.865540  1.274695   \n",
      "\n",
      "        7         8         9    ...       113       114       115       116  \\\n",
      "0 -1.385641 -0.805026 -0.051064  ...  0.182373  0.715759 -0.656651  0.454198   \n",
      "1 -1.020923 -0.455492 -0.424435  ... -1.174170 -0.181039 -0.766260 -0.603738   \n",
      "2  0.381704 -3.126015  1.302435  ... -0.228854  0.261778 -1.072097 -1.311487   \n",
      "3  1.371326 -4.050639  0.359183  ... -1.085560 -0.463217  0.620033 -0.025048   \n",
      "4 -0.308753 -3.052567  0.278035  ... -3.385667 -1.636756  2.325700 -0.382760   \n",
      "\n",
      "        117       118       119       120       121       122  \n",
      "0 -1.154797 -0.737946  0.463586 -0.162660 -0.648834  1.649686  \n",
      "1 -0.714217 -0.068665  0.935060  1.664834 -0.427394  0.809689  \n",
      "2  1.786036 -0.813005 -0.590950 -0.248594 -0.333682  0.111209  \n",
      "3 -0.232524  0.397717 -0.003491 -0.439884 -1.505008  0.131798  \n",
      "4 -0.710602  0.845203 -0.573920  1.216185  1.356412 -1.260814  \n",
      "\n",
      "[5 rows x 123 columns]\n",
      "Label Details:\n",
      "Shape: (1200, 1), Size: 1200\n",
      "Label Preview:\n",
      "  Transcription\n",
      "0      held out\n",
      "1      held out\n",
      "2      held out\n",
      "3      held out\n",
      "4      held out\n",
      "Sentence Lengths Details:\n",
      "Shape: (1200, 1), Size: 1200\n",
      "Sentence Lengths Preview:\n",
      "   Sentence Length\n",
      "0              224\n",
      "1              454\n",
      "2              216\n",
      "3              283\n",
      "4              243\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preview_data(partitioned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a15f9a-8e97-49ed-9e50-29de7a8d62d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:27:13.417411Z",
     "iopub.status.busy": "2024-05-29T18:27:13.417128Z",
     "iopub.status.idle": "2024-05-29T18:27:13.424737Z",
     "shell.execute_reply": "2024-05-29T18:27:13.423838Z",
     "shell.execute_reply.started": "2024-05-29T18:27:13.417398Z"
    }
   },
   "outputs": [],
   "source": [
    "char_vocab = set()\n",
    "for partition in partitioned_data:\n",
    "    for label in partitioned_data[partition]['label']:\n",
    "        char_vocab.update(label)\n",
    "char_vocab.add('_') \n",
    "char_vocab = sorted(char_vocab)\n",
    "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc122888-1d05-4222-a750-24d01a79acd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:27:13.426032Z",
     "iopub.status.busy": "2024-05-29T18:27:13.425833Z",
     "iopub.status.idle": "2024-05-29T18:27:13.439025Z",
     "shell.execute_reply": "2024-05-29T18:27:13.438168Z",
     "shell.execute_reply.started": "2024-05-29T18:27:13.426013Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, char_to_idx, idx_to_char):\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.pad_token_id = char_to_idx['_']\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded = [self.char_to_idx[char] for char in text]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, encoded):\n",
    "        decoded = ''.join([self.idx_to_char[idx] for idx in encoded])\n",
    "        return decoded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.char_to_idx)\n",
    "\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, input_size, num_channels, kernel_size, dropout):\n",
    "        super(TCNEncoder, self).__init__()\n",
    "        self.tcn = TCN(\n",
    "            num_inputs=input_size,\n",
    "            num_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            input_shape='NLC'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        return self.tcn(x)\n",
    "\n",
    "class BiGRUEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiGRUEncoder, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, _ = self.gru(x)\n",
    "        return outputs\n",
    "\n",
    "class SpeechBCIModel(nn.Module):\n",
    "    def __init__(self, input_size, tcn_num_channels, tcn_kernel_size, tcn_dropout, gru_hidden_size, vocab_size):\n",
    "        super(SpeechBCIModel, self).__init__()\n",
    "        self.tcn_encoder = TCNEncoder(input_size, tcn_num_channels, tcn_kernel_size, tcn_dropout)\n",
    "        self.gru_encoder = BiGRUEncoder(tcn_num_channels[-1], gru_hidden_size)\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "        self.projection = nn.Linear(2 * gru_hidden_size, self.t5_model.config.d_model)\n",
    "        self.output_layer = nn.Linear(self.t5_model.config.d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        input_ids = input_ids.unsqueeze(2)\n",
    "        tcn_outputs = self.tcn_encoder(input_ids)\n",
    "        gru_outputs = self.gru_encoder(tcn_outputs)\n",
    "        projected_embeddings = self.projection(gru_outputs)\n",
    "        attention_mask = (projected_embeddings != 0).any(dim=-1).float()\n",
    "        encoder_outputs = self.t5_model.encoder(\n",
    "            inputs_embeds=projected_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        logits = self.output_layer(encoder_outputs.last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "class SpeechBCIDataset(Dataset):\n",
    "    def __init__(self, features, labels, sen_lens):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.sen_lens = sen_lens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(char_tokenizer.encode(self.labels[idx]), dtype=torch.long)\n",
    "        sen_len = self.sen_lens[idx]\n",
    "        return feature, label, sen_len\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features, labels, sen_lens = zip(*batch)\n",
    "    max_sen_len = max(sen_lens)\n",
    "    padded_features = pad_sequence(features, batch_first=True, padding_value=0.0)\n",
    "    padded_labels = torch.full((len(labels), max_sen_len), char_tokenizer.pad_token_id, dtype=torch.long)\n",
    "    for i, label in enumerate(labels):\n",
    "        padded_labels[i, :len(label)] = label\n",
    "    sen_lens = torch.tensor(sen_lens)\n",
    "    return padded_features, padded_labels, sen_lens\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=char_tokenizer.pad_token_id)\n",
    "    for batch_features, batch_labels, batch_sen_lens in dataloader:\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        logits = model(input_ids=batch_features)\n",
    "        batch_size, seq_len, vocab_size = logits.size()\n",
    "        logits = logits.view(batch_size, seq_len, vocab_size)\n",
    "        batch_labels = batch_labels[:, :seq_len]\n",
    "        loss = criterion(logits.transpose(1, 2), batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    val_wer = 0.0\n",
    "    total_chars = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels, batch_sen_lens in dataloader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            logits = model(input_ids=batch_features)\n",
    "            batch_size, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch_size, seq_len, vocab_size)\n",
    "            generated_ids = logits.argmax(dim=-1)\n",
    "            generated_text = []\n",
    "            true_text = []\n",
    "            for i in range(batch_size):\n",
    "                gen_ids = generated_ids[i].cpu().numpy()\n",
    "                true_ids = batch_labels[i].cpu().numpy()\n",
    "                gen_ids = gen_ids[gen_ids != char_tokenizer.pad_token_id]\n",
    "                true_ids = true_ids[true_ids != char_tokenizer.pad_token_id]\n",
    "                generated_text.append(char_tokenizer.decode(gen_ids))\n",
    "                true_text.append(char_tokenizer.decode(true_ids))\n",
    "            for pred, true in zip(generated_text, true_text):\n",
    "                val_wer += editdistance.eval(pred, true)\n",
    "                total_chars += len(true)\n",
    "    val_wer /= total_chars\n",
    "    return val_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a2ea87-9530-42fd-885d-83716ea1c9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:27:13.440286Z",
     "iopub.status.busy": "2024-05-29T18:27:13.440118Z",
     "iopub.status.idle": "2024-05-29T18:33:36.272515Z",
     "shell.execute_reply": "2024-05-29T18:33:36.271635Z",
     "shell.execute_reply.started": "2024-05-29T18:27:13.440268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.5097, Val WER = 0.9728\n",
      "Epoch 2: Train Loss = 2.4803, Val WER = 0.9730\n",
      "Epoch 3: Train Loss = 2.4769, Val WER = 0.9730\n",
      "Epoch 4: Train Loss = 2.4739, Val WER = 0.9730\n",
      "Epoch 5: Train Loss = 2.4730, Val WER = 0.9730\n",
      "Epoch 6: Train Loss = 2.4693, Val WER = 0.9710\n",
      "Epoch 7: Train Loss = 2.4627, Val WER = 0.9730\n",
      "Epoch 8: Train Loss = 2.4477, Val WER = 0.9730\n",
      "Epoch 9: Train Loss = 2.4262, Val WER = 0.9729\n",
      "Epoch 10: Train Loss = 2.3826, Val WER = 0.9726\n",
      "Epoch 11: Train Loss = 2.3377, Val WER = 0.9729\n",
      "Epoch 12: Train Loss = 2.2706, Val WER = 0.9732\n",
      "Epoch 13: Train Loss = 2.1925, Val WER = 0.9738\n",
      "Epoch 14: Train Loss = 2.1093, Val WER = 0.9743\n",
      "Epoch 15: Train Loss = 2.0255, Val WER = 0.9754\n",
      "Epoch 16: Train Loss = 1.9276, Val WER = 0.9740\n",
      "Epoch 17: Train Loss = 1.8356, Val WER = 0.9769\n",
      "Epoch 18: Train Loss = 1.7266, Val WER = 0.9756\n",
      "Epoch 19: Train Loss = 1.6336, Val WER = 0.9767\n",
      "Epoch 20: Train Loss = 1.5387, Val WER = 0.9766\n",
      "Epoch 21: Train Loss = 1.4470, Val WER = 0.9749\n",
      "Epoch 22: Train Loss = 1.3651, Val WER = 0.9753\n",
      "Epoch 23: Train Loss = 1.2843, Val WER = 0.9751\n",
      "Epoch 24: Train Loss = 1.2292, Val WER = 0.9763\n",
      "Epoch 25: Train Loss = 1.1638, Val WER = 0.9755\n",
      "Epoch 26: Train Loss = 1.1096, Val WER = 0.9755\n",
      "Epoch 27: Train Loss = 1.0494, Val WER = 0.9770\n",
      "Epoch 28: Train Loss = 1.0341, Val WER = 0.9761\n",
      "Epoch 29: Train Loss = 0.9865, Val WER = 0.9762\n",
      "Epoch 30: Train Loss = 0.9622, Val WER = 0.9756\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "input_size = partitioned_data[\"train\"][\"features\"].shape[1]\n",
    "tcn_hidden_size = 128\n",
    "tcn_num_channels = [tcn_hidden_size, tcn_hidden_size, tcn_hidden_size * 2]\n",
    "tcn_kernel_size = 3\n",
    "tcn_dropout = 0.1\n",
    "gru_hidden_size = tcn_hidden_size * 2\n",
    "vocab_size = len(char_vocab)\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 30\n",
    "batch_size = 16\n",
    "max_seq_length = 128\n",
    "\n",
    "# Initialize character tokenizer\n",
    "char_tokenizer = CharTokenizer(char_to_idx, idx_to_char)\n",
    "\n",
    "# Initialize model, optimizer, loss function, and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SpeechBCIModel(\n",
    "    input_size=input_size,\n",
    "    tcn_num_channels=tcn_num_channels,\n",
    "    tcn_kernel_size=tcn_kernel_size,\n",
    "    tcn_dropout=tcn_dropout,\n",
    "    gru_hidden_size=gru_hidden_size,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = SpeechBCIDataset(partitioned_data[\"train\"][\"features\"], partitioned_data[\"train\"][\"label\"], partitioned_data[\"train\"][\"sen_len\"])\n",
    "val_dataset = SpeechBCIDataset(partitioned_data[\"test\"][\"features\"], partitioned_data[\"test\"][\"label\"], partitioned_data[\"test\"][\"sen_len\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss = float(\"inf\")\n",
    "patience = 4\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    val_wer = evaluate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val WER = {val_wer:.4f}\")\n",
    "\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    scheduler.step(train_loss)        \n",
    "torch.save(model.state_dict(), 'best_model.pth')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78ff0ab0-06c2-4feb-8dc4-9cefdd4e0a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:33:36.274804Z",
     "iopub.status.busy": "2024-05-29T18:33:36.274577Z",
     "iopub.status.idle": "2024-05-29T18:33:36.281597Z",
     "shell.execute_reply": "2024-05-29T18:33:36.280683Z",
     "shell.execute_reply.started": "2024-05-29T18:33:36.274785Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wer(decoded_sentences, true_sentences):\n",
    "    total_word_errors = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for decoded_sent, true_sent in zip(decoded_sentences, true_sentences):\n",
    "        decoded_words = decoded_sent.split(\" \")\n",
    "        true_words = true_sent.split(\" \")\n",
    "        word_errors = editdistance.eval(decoded_words, true_words)\n",
    "        total_word_errors += word_errors\n",
    "        total_words += len(true_words)\n",
    "\n",
    "    wer = total_word_errors / total_words\n",
    "    return wer\n",
    "\n",
    "def inference(model, dataset, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        feature, label, sen_len = dataset[idx]\n",
    "        feature = feature.unsqueeze(0).to(device)\n",
    "        logits = model(input_ids=feature)\n",
    "        generated_ids = logits.argmax(dim=-1)\n",
    "        generated_text = char_tokenizer.decode(generated_ids[0].cpu().numpy())\n",
    "        true_text = char_tokenizer.decode(label.numpy())\n",
    "        print(\"Generated Text:\", true_text)\n",
    "        print(\"Ground Truth:\", true_text)\n",
    "\n",
    "def calculate_wer_on_val_set(model, val_dataset, device):\n",
    "    model.eval()\n",
    "    decoded_sentences = []\n",
    "    true_sentences = []\n",
    "    with torch.no_grad():\n",
    "        for feature, label, sen_len in val_dataset:\n",
    "            feature = feature.unsqueeze(0).to(device)\n",
    "            logits = model(input_ids=feature)\n",
    "            generated_ids = logits.argmax(dim=-1)\n",
    "            generated_text = char_tokenizer.decode(generated_ids[0].cpu().numpy())\n",
    "            true_text = char_tokenizer.decode(label.numpy())\n",
    "            decoded_sentences.append(generated_text)\n",
    "            true_sentences.append(true_text)\n",
    "    wer = calculate_wer(decoded_sentences, true_sentences)\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c8de5f8-9673-4b77-b0b4-9a992401d3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:33:36.282763Z",
     "iopub.status.busy": "2024-05-29T18:33:36.282530Z",
     "iopub.status.idle": "2024-05-29T18:33:41.370055Z",
     "shell.execute_reply": "2024-05-29T18:33:41.369374Z",
     "shell.execute_reply.started": "2024-05-29T18:33:36.282742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation WER: 0.9909074377159484\n"
     ]
    }
   ],
   "source": [
    "# Load the best model checkpoint\n",
    "best_model_path = 'best_model.pth'\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Calculate WER on the entire validation set\n",
    "val_wer = calculate_wer_on_val_set(model, val_dataset, device)\n",
    "print(\"Validation WER:\", val_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8611b0b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T18:33:41.371186Z",
     "iopub.status.busy": "2024-05-29T18:33:41.370959Z",
     "iopub.status.idle": "2024-05-29T18:33:41.383819Z",
     "shell.execute_reply": "2024-05-29T18:33:41.382966Z",
     "shell.execute_reply.started": "2024-05-29T18:33:41.371166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: i don't see much tv mostly when i'm in school.\n",
      "Ground Truth: i don't see much tv mostly when i'm in school.\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on a random sample from the validation set\n",
    "inference(model, val_dataset, device)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
